:toc:


== Explanation of similarity search

We have a set of documents, each document is a set of words.
Nowadays, all meta information is stored in a database.
But the documents are stored in the Elasticsearch.
It's well described in the following paper:
https://github.com/arachnidllc/BillMap/blob/main/architecture.jpg[here]

* Firstly we get data from congress.gov.
* Ananlyze data step by step:
* Get the data from congress.gov.
* Write bill ids and bill titles to a database.
* Analyze bill meta information.
* Analyze related bills
* Store the documents in Elasticsearch (bill text and bill sections)
* Analyze similarity between bills
* Get newly loaded bills from Databsase
* Compare bills using https://github.com/aih/billsim[billsim] GO package
* For each bill, get the documents from Elasticsearch using MLT (More Like This) query
* Store the results in a database
* Get the results from the database

== SQL Approach to find similar documents over the set of documents

=== Build needed indexes (GIN, GIST)
The main trick here is to build correct indexes.
Index should be built on the basis of the documents and should contain n-grams

`TODO`: we should decide the length of n-grams.

Having indexes sorted in right direction and build with correct mechanisms and tools we can use them as if all data is just loaded in memory.

We are building using `GIN` of `GIST` indexes in PostgreSQL.

This command creates an index consisting of title `id`, `title` as `gin_trgm_ops` (trigrams) and `text` as char trigrams and `title` as tsvector (vectorized representations of the title).

There's a tricky thing that we're saving not exactly n-grams, but prepared query for full text search based on n-grams.

To generate n-grams we can use `nltk` package (and in that case we will need to calculate them each time for each row while we load it into the DB). I also implemented it in pure SQL:

=== Create function to calculate n-grams

[source,sql]
----
/*
 * Calculate n-grams for a given text
 * Utility function to generate n-grams from a string.
 * Returns tsquery string with n-grams.
 * @param land - language to use to generate n-grams efficiently
 * @param text - text to calculate n-grams for
 * @param n - n-gram length
 * @returns - n-grams as tsquery
*/
CREATE OR REPLACE FUNCTION phrase_ngram(lng regconfig, t text, n int)
    RETURNS tsquery
    LANGUAGE plpgsql
    IMMUTABLE AS
$$
DECLARE
    words  text[];
    i      integer;
    result tsquery;
    q      tsquery;
BEGIN
    /* split the string into an array of words */
    words := regexp_split_to_array(lower($2), '[^[:alnum:]]+');
    for i in 1 .. cardinality(words) - n + 1
        LOOP
        /* a phrase consisting of n consecutive words */
        q := phraseto_tsquery($1, array_to_string(words[i : i + n - 1], ' '));
        IF result IS NULL THEN
            result := q;
        ELSE
            /* append with "or" */
            result := result || q;
        END IF;
    END LOOP;
    /*
    ToDo: Select only unique n-grams from generated tsquery
    */
    RETURN result;
END;
$$;
----

This function splits the text into words and generates n-grams.
To test it you can run this command to generate 4-grams:

[source,sql]
-----
SELECT phrase_ngram('english', 'To extend the registration and reporting requirements of the Federal securities laws to certain housing-related Government-sponsored enterprises, and for other purposes. ', 4);
-----

Result:

[source]
----
'extend' <2> 'registr' | 'extend' <2> 'registr' | 'registr' <2> 'report' | 'registr' <2> 'report' <-> 'requir' | 'report' <-> 'requir' | 'report' <-> 'requir' | 'requir' <3> 'feder' | 'feder' <-> 'secur' | 'feder' <-> 'secur' <-> 'law' | 'feder' <-> 'secur' <-> 'law' | 'secur' <-> 'law' <2> 'certain' | 'law' <2> 'certain' <-> 'hous' | 'certain' <-> 'hous' <-> 'relat' | 'certain' <-> 'hous' <-> 'relat' <-> 'govern' | 'hous' <-> 'relat' <-> 'govern' <-> 'sponsor' | 'relat' <-> 'govern' <-> 'sponsor' <-> 'enterpris' | 'govern' <-> 'sponsor' <-> 'enterpris' | 'sponsor' <-> 'enterpris' | 'enterpris' | 'purpos' | 'purpos'
----

Where:

* n-grams are separated by `&nbsp;|&nbsp;` symbol (logical operator `OR`)
* words are normalised
* `<n>`  - how many words were between n-gram words before https://en.wikipedia.org/wiki/Stemming[stemming]
* <-> means that words should be linked (actually means that it's n-gram)
* it's standard PostgreSQL full text search query mechanism and it is described here: https://www.postgresql.org/docs/9.6/static/textsearch-controls.html[here]


=== Query explanation ===


We need these fields to be stored in the indexes:

* `id` title ID
* `title` split in 3-gram (`gin_trgm_ops` OR `gist_trgm_ops`)
* `title_ngram` split in 4-gram (`gin_trgm_ops` OR `gist_trgm_ops`)

to speed up this part of query:

[source,sql]
----
    from left_titles lt, right_titles rt
        /*
        Requires composite index for fields:
        id, title gin_trgm, to_tsvector(title)
        */
        WHERE true
            /* No need to check. Score always eqals 1 for same doccuments */
            and lt.id <> rt.id
            /* Possibly might decrease calculation time:
            and lt.id > rt.id
            */
            /* Filter by trigrams hash firstly */
            and lt.title % rt.title
            /* If trigram hash comparison returns values greater than 0.5,
            then do full text search:
            from left to right and vice versa.
            TODO: investigate if we can decrease the number of rows to be processed.
            */
            and (
                to_tsvector(rt.title) @@ lt.title_n_grams::tsquery
                or to_tsvector(lt.title) @@ rt.title_n_grams::tsquery
            )
----

This actually makes cartesian product of all titles filtered in 2 (might be useful to apply one more additional filter) stages (trigrams and full text search).

Where:

* 1st stage:

[source,sql]
----
lt.title % rt.title
----

Under the hood of postgresql operator `%` is a bitwise operator.
It checks n-grams similarity with threshold 0.3 (threshold can be changed easily)

* 1.a `possible` stage to filter more precisely (not tested well yet on my side, but IMO it should help):

[source,sql]
----
(
       lt.title %>> rt.title
    or rt.title %>> lt.title
)
----

* 2nd stage will be executed only if the first has passed successfully and returned `TRUE` checks for trigrams hash equality using full text search.

[source,sql]
----
and (
       to_tsvector(rt.title) @@ lt.title_n_grams::tsquery
    or to_tsvector(lt.title) @@ rt.title_n_grams::tsquery
)
----


The next stage is to calculate the score of the match.

Basically it can be achieved by calculating the number of intersected n-grams between texts divided by the length of n-grams of what item we compare.

[source,math]
----
left_to_right_score = len(n_grams_intersection) / len(left_n_grams)

right_to_left_score = len(n_grams_intersection) / len(right_n_grams)
----

In this approach to calculate ranks (score) we are using the following piece of code:

[source,sql]
-------
ts_rank_cd(
        to_tsvector('english', rt),
        to_tsquery('english', lt_n_g),
        32
    )          AS ltr_rank,
ts_rank_cd(
    to_tsvector('english', lt),
    to_tsquery('english', rt_n_g),
    32
)              AS rtl_rank
-------

where:

Function ts_rank_cd is a function that calculates the rank of the text compared to the query. It has this notation:

[source,pseudocode]
----
ts_rank_cd([ weights float4[], ] vector tsvector, query tsquery [, normalization integer ]) returns float4
----

Where:

* vector - vectorized text
* tsquery - query
* normalization - bit mask normalization factor (we are using 32, but it's pretty flexible)

Possible values of normalization are:

* 0 (the default) ignores the document length
* 1 divides the rank by 1 + the logarithm of the document length
* 2 divides the rank by the document length
* 4 divides the rank by the mean harmonic distance between extents (this is implemented only by ts_rank_cd)
* 8 divides the rank by the number of unique words in document
* 16 divides the rank by 1 + the logarithm of the number of unique words in document
* 32 divides the rank by itself + 1

If more than one flag bit is specified, the transformations are applied in the order listed.

How to configure rank calculation is described https://www.postgresql.org/docs/9.6/static/textsearch-controls.html[here].


== Google document with explanation and some analysis

The whole query with comments is stored in google sheet https://docs.google.com/spreadsheets/d/1-VYuSP9_2-dkRCVffQX9rpJp5jELUL6DiACZ2RKIMYk/edit?usp=sharing[here].

* `Titles full text search by ngrams (fast)` tab explains the fastest algorithm to compare all to all (it could be improved further)

* `Titles n-grams as arrays (slow)` tab explains the first approach. I don't think we'll use it in the future, but it explains the whole algorithm in details.
* `Length of text vs. Titles to Compare` tab has self-descriptive name
* Last tab - `Bills Statistic` - small statistic about bills length

== Base query explanation ==

Basically the query does the following:

* Build matrix of full union of all documents to all documents

* Apply pre-filter by trigram similarity (each trigram consists of 3 chars)

* Apply post-filter by trigram similarity (each trigram consists of 3 words build as ts_query)


Code needs to be executed in psql console:

[source,sql]
----
/*
Needed extensions
*/
/*
https://www.postgresql.org/docs/current/pgprewarm.html
*/
CREATE EXTENSION pg_prewarm;
/*
https://www.postgresql.org/docs/9.0/pgtrgm.html
*/
CREATE EXTENSION pg_trgm;
/*
https://www.postgresql.org/docs/current/btree-gin.html
*/
CREATE EXTENSION btree_gin;

/*
Postgresql settigns tweaks
*/
/* should be sst to value of 1/2 of total RAM memory */
SET effective_cache_size = '14 GB';

/*
https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-WORK-MEM
Not sure about this setting, but it also might be useful
*/
show work_mem;
/* Uncomment next line to change work_mem to 100MB */
-- set work_mem = '100MB';
/*
https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-WORK-MEM
*/
show hash_mem_multiplier;
set hash_mem_multiplier = 2.0;

/*
Utility function to generate n-grams from a string.
Returns tsquery string with n-grams.
*/
CREATE OR REPLACE FUNCTION phrase_ngram(lng regconfig, t text, n int)
    RETURNS tsquery
    LANGUAGE plpgsql
    IMMUTABLE AS
$$
DECLARE
    words  text[];
    i      integer;
    result tsquery;
    q      tsquery;
BEGIN
    /* split the string into an array of words */
    words := regexp_split_to_array(lower($2), '[^[:alnum:]]+');
    for i in 1 .. cardinality(words) - n + 1
        LOOP
        /* a phrase consisting of n consecutive words */
        q := phraseto_tsquery($1, array_to_string(words[i : i + n - 1], ' '));
        IF result IS NULL THEN
            result := q;
        ELSE
            /* append with "or" */
            result := result || q;
        END IF;
    END LOOP;
    /*
    ToDo: Select only unique n-grams from generated tsquery
    */
    RETURN result;
END;
$$;

/* Add fields to billstagetitle table */
alter table btiapp_billstagetitle
add column title_n_grams text;

/*
Populate billstagetitle.title_n_grams field with generated n-grams.
NOTE: it will try to generate up to 8-grams.
If value for 8-gram is empty it will try to
generate n-grams for n from 8 down to 1.
*/
update btiapp_billstagetitle
set title_n_grams=COALESCE(
    phrase_ngram('english'::regconfig, title, 8),
    phrase_ngram('english'::regconfig, title, 7),
    phrase_ngram('english'::regconfig, title, 6),
    phrase_ngram('english'::regconfig, title, 5),
    phrase_ngram('english'::regconfig, title, 4),
    phrase_ngram('english'::regconfig, title, 3),
    phrase_ngram('english'::regconfig, title, 2),
    phrase_ngram('english'::regconfig, title, 1)
)
where true;


/*
Indexes
*/
/* n-gram length index */
CREATE INDEX title_ngram_length_idx
    on btiapp_billstagetitle (
        cardinality(
        regexp_split_to_array(
        title_n_grams, '\|'
        )
    ) desc
);
/* composite index  id, title gin_trgm_ops, to_tsvector('english', title) */
CREATE INDEX id_title_trgm_title_ts_idx ON btiapp_billstagetitle
    USING GIN (id, title gin_trgm_ops, to_tsvector('english', title));


/*
FIND SIMILAR TITLES
and store them in a materialized view
*/
-- CREATE MATERIALIZED VIEW stage_title_compared_mv AS
WITH
/*
Load data to the PG cache to warm up the index.
*/
    warm_up as (
        select (
            (select pg_prewarm('btiapp_billstagetitle')) +
            (select pg_prewarm('title_ngram_length_idx')) +
            (select pg_prewarm('id_title_trgm_title_ts_idx'))
        ) as pre_warmed_blocks
    ),
    limit_num as (
        /*
        Limit the number of rows to be processed.
        */
        select 100 as n
        /* To process all rows uncomment the following line
        select (select count(*) from btiapp_billstagetitle) as n
        */
    ),
    offset_num as (select 0 as n),
    left_titles AS (
        SELECT id,
        bill_basic_id,
        title,
        title_word_ngrams,
        title_ngrams_length,
        title_n_grams
        FROM btiapp_billstagetitle
        /* To play with specific bill id uncomment the following line
        where id = '{bill_id}'
        */

        /* Uses the index title_ngram_length_idx */
        order by cardinality(regexp_split_to_array(title_n_grams, '\|')) desc
        limit (select n from limit_num) offset (select n from offset_num)
    ),
    right_titles AS (
        SELECT id, bill_basic_id, title, title_word_ngrams, title_n_grams, title_ngrams_length
        FROM btiapp_billstagetitle
    ),
    p as (
        select
            lt.id                                                      as lt_id,
            rt.id                                                      as rt_id,
            lt.bill_basic_id                                           as lb_id,
            rt.bill_basic_id                                           as rb_id,
            lt.title                                                   as lt,
            rt.title                                                   as rt,
            lt.title_n_grams                                           as lt_n_g,
            rt.title_n_grams                                           as rt_n_g,
            concat(GREATEST(lt.id, rt.id), '<->', LEAST(lt.id, rt.id)) as uid
        from left_titles lt, right_titles rt
        /*
        Requires composite index for fields:
        id, title gin_trgm, to_tsvector(title)
        */
        WHERE true
            and lt.id <> rt.id
            /* Possibly might decrease calculation time:
            and lt.id > rt.id
            */
            /* Filter by trigrams hash firstly */
            and lt.title % rt.title
            /* If trigram hash comparison returns values greater than 0.5,
            then do full text search:
            from left to right and vice versa.
            TODO: investigate if we can decrease the number of rows to be processed.
            */
            and (
                to_tsvector(rt.title) @@ lt.title_n_grams::tsquery
                or to_tsvector(lt.title) @@ rt.title_n_grams::tsquery
            )
    )
select
    (select pre_warmed_blocks from warm_up),
    uid,
    lt_id,
    rt_id,
    /*
    https://www.postgresql.org/docs/current/textsearch-controls.html#TEXTSEARCH-RANKING
    */
    ts_rank_cd(
        to_tsvector('english', rt),
        to_tsquery('english', lt_n_g),
        32
    )              AS ltr_rank,
    ts_rank_cd(
        to_tsvector('english', lt),
        to_tsquery('english', rt_n_g),
        32
    )              AS rtl_rank,
    /*
    https://www.postgresql.org/docs/9.0/pgtrgm.html
    */
    similarity(lt, rt) AS similarity_score
from p
order by ltr_rank desc, rtl_rank desc, similarity_score desc;


/*
To get title with highlighted similarities
*/
WITH comp as (
    select *
        from stage_title_compared_mv
        where
        --           similarity_score > 0.8 order by similarity_score limit 1
    )
select comp.uid,
    comp.ltr_rank,
    comp.rtl_rank,
    comp.similarity_score,
    lt.title,
    rt.title,
    ts_headline(
        'english', lt.title,
        to_tsquery('english', rt.title_n_grams)
        , 'HighlightAll=true'
    ) as lt_headline,
    ts_headline(
        'english', rt.title,
        to_tsquery('english', lt.title_n_grams)
        , 'HighlightAll=true'
    ) as rt
from comp
join btiapp_billstagetitle lt on lt.id = comp.lt_id
join btiapp_billstagetitle rt on rt.id = comp.rt_id;


/*
To check the percentage of similarity all to all
*/
select total_rows,
    left_cnt,
    right_cnt,
    left_cnt * right_cnt as total_pairs,
    CONCAT(ROUND((total_rows::numeric / (left_cnt * right_cnt)) * 100, 2), '%') as total_pairs_percentage
from (
    select (select count(*) from stage_title_compared_mv) as total_rows,
    (select count(*)
    from (select count(rt_id) from stage_title_compared_mv group by rt_id) t
    ) left_cnt,
    (select count(*)
    from (select count(lt_id) from stage_title_compared_mv group by lt_id) t
    ) as right_cnt
) t;
----


=== Possible improvements ===

Next what we will possibly need is to create partial indexes to speed up queries, but it could take some time to investigate data which we have and how correctly we can divide it in partials.

Also, we possibly will need to think about length of n-grams. Now I'm trying to create 8-gram. If ngram was not generated it will generate smaller one and down to 1-gram.
that's because titles can be small
for bills we possibly won't need 1-grams
